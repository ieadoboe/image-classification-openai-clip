# Image Classification with OpenAI's Contrastive Language-Image Pretraining (CLIP) model

CLIP (Contrastive Language-Image Pretraining) is a cutting-edge model from OpenAI that enables image classification without explicit training on specific categories.

- CLIP official link: [CLIP: Connecting text and images - Jan 5, 2021](https://openai.com/index/clip/)
- Paper: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- Code: [CLIP - GitHub](https://github.com/openai/CLIP)

## Project Scope

- Use OpenAI’s CLIP model to classify images based on text descriptions.
- Implement a web app where users upload an image and describe what they want to classify.
- Compare CLIP’s performance against a traditional CNN trained on a subset of ImageNet.

## Skills Showcased

- Vision-Language models
- Transfer learning
- Model evaluation.

## Focus

This project focuses on implementing and fine-tuning OpenAI's model for practical applications.
