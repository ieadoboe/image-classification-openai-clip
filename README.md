# Image Classification with OpenAI's Contrastive Language-Image Pretraining (CLIP) model

---

CLIP (Contrastive Language-Image Pretraining) is a cutting-edge model from OpenAI that enables image classification without explicit training on specific categories.

## Project Scope

- Use OpenAI’s CLIP model to classify images based on text descriptions.
- Implement a web app where users upload an image and describe what they want to classify.
- Compare CLIP’s performance against a traditional CNN trained on a subset of ImageNet.

## Skills Showcased

- Vision-Language models
- Transfer learning
- Model evaluation.

## Feasibility

CLIP is pre-trained, so the heavy lifting is already done—your focus will be on implementing and fine-tuning it for practical applications.
